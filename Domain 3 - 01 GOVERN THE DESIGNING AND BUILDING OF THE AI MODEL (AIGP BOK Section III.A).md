# Domain 2: Understanding How Laws, Standards, and Frameworks Apply to AI

## IV. MAIN INDUSTRY STANDARDS AND TOOLS. (AIGP BOK Section II.D)

**Status:**: Complete<br>
**Target:** `12-Feb-2026



### A. OECD Principles, Framework, Policies, and Recommended Practices for Trustworthy AI

#### 1. Core OECD AI Principles

- Inclusive Growth, Sustainable Development, and Well-Being:
- Human-Centered Values and Fairness
- Transparency and Explainability
- Robustness, Security, and Safety
- Accountability

#### 2. Framework and Policies for Implementation

- National AI Strategies
- Research and Collaboration
- Workforce Development
- Public Trust and Engagement
- Data and Infrastructure Access

**Exam Tip**: If a scenario describes national AI strategies, workforce training, or international cooperation, this is commonly treated on the exam as part of the OECD’s implementation framework. Answers in these situations often emphasize its five pillars: strategies, research, workforce, trust, and infrastructure.

- Ethical Review Boards for AI Projects
- Impact assessments
- Documentation tools
- Transparency reporting
- Auditability and redress mechanisms

**Exam Tips for OECD Principles:** 

- OECD = first global AI principles (2019), endorsed by G20. 
- Core 5 principles = IHTRA → Inclusive, Human-centered, Transparency, Robustness, Accountability. 
- Scope: OECD principles are high-level and non-binding, but they strongly influence national AI strategies and frameworks (e.g., EU AI Act, U.S. National AI Initiative). 



### B. NIST AI Risk Management Framework & Playbook

The **National Institute of Standards and Technology (NIST)** released the AI Risk Management Framework (AI RMF) in 2023 as a voluntary but widely adopted guide for managing AI risks. Much like the NIST Cybersecurity Framework, it has become a global reference point, offering both principles and practical tools. The framework is organized into four core functions: **Govern, Map, Measure, and Manage**, each with categories and subcategories that operationalize responsible AI. These are further supported by the NIST AI RMF Playbook, which provides detailed practices, examples, and implementation guidance.

<img src="https://go.forrester.com/wp-content/uploads/2023/02/2CD1E730-D45F-4C17-8CB1-B862FCC7523B_1_201_a.jpeg" style="zoom:50%;" />

- **Govern:** This core function establishes the foundation of an effective AI risk management program. It is about creating the culture, structures, and accountability mechanisms that make responsible AI possible.
- **Map:** This core function focuses on situational awareness. Before risks can be managed, the organization must understand the system’s intended purpose, functions, and boundaries. That means defining assumptions and limitations at the outset and documenting them for clarity.
- **Measure:** This core function ensures that AI systems are evaluated against clear criteria and that those evaluations are continuous. Metrics and indicators establish benchmarks for reliability, robustness, fairness, transparency, and security; for example, monitoring bias metrics over time to detect drift.
- **Manage:** This core function is where insights from governance, mapping, and measurement translate into sustained action. Risk prioritization ensures that issues are ranked by severity and likelihood, maintained in a risk register that informs decisions. Mitigation planning assigns concrete steps to reduce risk and names accountable owners.

Alongside the framework itself, NIST provides a companion resource known as the **NIST AI RMF** Playbook. This practical guide is designed to help organizations move from theory to application by offering concrete examples of how to implement each subcategory in the framework.

The NIST AI RMF is especially important for AIGP exam preparation because it reflects how the U.S. approaches AI governance and risk management. Unlike purely technical standards, the RMF takes a socio-technical perspective, recognizing that AI risks are not just about system accuracy or robustness, but also about fairness, explainability, accountability, and broader societal impacts.



### C. NIST ARIA Program - REVIEW

The **NIST Assessing Risks and Impacts of AI (ARIA)** Program is designed to complement the AI Risk Management Framework (AI RMF) by moving from principles to practice. While the AI RMF outlines what organizations should govern, **ARIA addresses how to measure those obligations in a concrete, scientific way.** - https://ai-challenges.nist.gov/aria

By grounding governance in measurement science, ARIA strengthens the broader regulatory and policy landscape. Its role is to ensure that AI accountability is not just aspirational but evidence-based, enabling governments and organizations to rely on verifiable standards in procurement, oversight, and international coordination.

**Exam Tips for NIST ARIA:** 

- ARIA = technical measurements for AI safety, robustness, fairness, and explainability. 

- Think of it as RMF’s companion: RMF = governance framework; ARIA = testing + metrics. 
- Exam trigger: If question = “which NIST program develops metrics and methodologies for AI trustworthiness?” → answer = ARIA.



### D. ISO Core AI Standards

#### 1. ISO/IEC 22989 — AI Concepts and Terminology

ISO/IEC 22989 establishes the common vocabulary and conceptual framework for artificial intelligence. Its primary purpose is to make sure that when governments, industries, and regulators talk about AI, they are speaking the same language.

Governance Implication: Shared terminology is not just academic, it has real-world impact. Clear definitions ensure consistency in contracts, compliance reviews, and regulatory enforcement, especially in cross-border contexts where multiple legal systems must align.

#### 2. ISO/IEC 42001 — AI Management System Standard (AIMS)

ISO/IEC 42001, published in 2023, is the world’s first certifiable AI management system standard. Much like ISO/IEC 27001 sets the benchmark for information security management, 42001 provides a structured way for organizations to build, implement, and demonstrate responsible AI governance.

This standard is expected to serve as a benchmark for compliance under the EU AI Act and other emerging regulatory regimes, making it one of the most influential AI governance tools globally.

**Exam Tip**: If a question asks why 22989 and 42001 are “core” ISO standards, remember the pairing: 22989 = language, 42001 = framework. Together they deliver both clarity and accountability. 

- Exam Tips for ISO Core AI Standards: 22989 = Terminology. 42001 = Certifiable AI governance (AIMS). 

- If exam asks “which ISO AI standard is certifiable?” → answer = 42001. 
- Mnemonic: “T & C” = Terminology (22989) + Certification (42001). 
- Scenario flag: If a question references audits or certification of AI governance, the right answer is ISO/IEC 42001.





















